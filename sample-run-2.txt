(letta) kyuds@kyuds paperbot % just run
python main.py
Successfully found agent: agent-73f44d42-0d08-4708-b1cf-da34a3db3667
Batches: 100%|████████████████████████████████████| 7/7 [00:02<00:00,  2.51it/s]
Query Response: id='message-09f24d8d-1354-4a4c-9825-60285890e9f4' date=datetime.datetime(2025, 3, 10, 12, 42, 28, tzinfo=TzInfo(UTC)) message_type='reasoning_message' reasoning='Creating a query string for a vector similarity search, incorporating synonyms and concepts related to agent systems and prompt engineering while avoiding biology-related terms.'

Query Response: id='message-09f24d8d-1354-4a4c-9825-60285890e9f4' date=datetime.datetime(2025, 3, 10, 12, 42, 28, tzinfo=TzInfo(UTC)) message_type='assistant_message' content='"large language models, agent systems, prompt engineering frameworks, interactive AI, human-AI collaboration, intelligent prompting techniques, adaptive prompting, information retrieval methodologies"'

Top 5 Response: id='message-c2da9308-c84a-4199-b562-e52c924dc6d0' date=datetime.datetime(2025, 3, 10, 12, 42, 32, tzinfo=TzInfo(UTC)) message_type='reasoning_message' reasoning="Now selecting the 5 most relevant papers based on Daniel's interests in agent systems, prompt engineering, and avoiding biology-related papers."

Top 5 Response: id='message-c2da9308-c84a-4199-b562-e52c924dc6d0' date=datetime.datetime(2025, 3, 10, 12, 42, 32, tzinfo=TzInfo(UTC)) message_type='assistant_message' content='1 3 6 7 8'

Recommended:
Prompt Programming: A Platform for Dialogue-based Computational Problem Solving with Generative AI Models

Computing students increasingly rely on generative AI tools for programming assistance, often without formal instruction or guidance. This highlights a need to teach students how to effectively interact with AI models, particularly through natural language prompts, to generate and critically evaluate code for solving computational tasks. To address this, we developed a novel platform for prompt programming that enables authentic dialogue-based interactions, supports problems involving multiple interdependent functions, and offers on-request execution of generated code. Data analysis from over 900 students in an introductory programming course revealed high engagement, with the majority of prompts occurring within multi-turn dialogues. Problems with multiple interdependent functions encouraged iterative refinement, with progression graphs highlighting several common strategies. Students were highly selective about the code they chose to test, suggesting that on-request execution of generated code promoted critical thinking. Given the growing importance of learning dialogue-based programming with AI, we provide this tool as a publicly accessible resource, accompanied by a corpus of programming problems for educational use.

http://arxiv.org/abs/2503.04267v1

Measuring temporal effects of agent knowledge by date-controlled tool use

Temporal progression is an integral part of knowledge accumulation and update. Web search is frequently adopted as grounding for agent knowledge, yet its inappropriate configuration affects the quality of agent responses. Here, we construct a tool-based out-of-sample testing framework to measure the knowledge variability of large language model (LLM) agents from distinct date-controlled tools (DCTs). We demonstrate the temporal effects of an LLM agent as a writing assistant, which can use web search to help complete scientific publication abstracts. We show that temporal effects of the search engine translates into tool-dependent agent performance but can be alleviated with base model choice and explicit reasoning instructions such as chain-of-thought prompting. Our results indicate that agent evaluation should take a dynamical view and account for the temporal influence of tools and the updates of external resources.

http://arxiv.org/abs/2503.04188v1

ToolFuzz -- Automated Agent Tool Testing

Large Language Model (LLM) Agents leverage the advanced reasoning capabilities of LLMs in real-world applications. To interface with an environment, these agents often rely on tools, such as web search or database APIs. As the agent provides the LLM with tool documentation along the user query, the completeness and correctness of this documentation is critical. However, tool documentation is often over-, under-, or ill-specified, impeding the agent's accuracy. Standard software testing approaches struggle to identify these errors as they are expressed in natural language. Thus, despite its importance, there currently exists no automated method to test the tool documentation for agents. To address this issue, we present ToolFuzz, the first method for automated testing of tool documentations. ToolFuzz is designed to discover two types of errors: (1) user queries leading to tool runtime errors and (2) user queries that lead to incorrect agent responses. ToolFuzz can generate a large and diverse set of natural inputs, effectively finding tool description errors at a low false positive rate. Further, we present two straightforward prompt-engineering approaches. We evaluate all three tool testing approaches on 32 common LangChain tools and 35 newly created custom tools and 2 novel benchmarks to further strengthen the assessment. We find that many publicly available tools suffer from underspecification. Specifically, we show that ToolFuzz identifies 20x more erroneous inputs compared to the prompt-engineering approaches, making it a key component for building reliable AI agents.

http://arxiv.org/abs/2503.04479v1

Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert Elicitation

The literature and multiple experts point to many potential risks from large language models (LLMs), but there are still very few direct measurements of the actual harms posed. AI risk assessment has so far focused on measuring the models' capabilities, but the capabilities of models are only indicators of risk, not measures of risk. Better modeling and quantification of AI risk scenarios can help bridge this disconnect and link the capabilities of LLMs to tangible real-world harm. This paper makes an early contribution to this field by demonstrating how existing AI benchmarks can be used to facilitate the creation of risk estimates. We describe the results of a pilot study in which experts use information from Cybench, an AI benchmark, to generate probability estimates. We show that the methodology seems promising for this purpose, while noting improvements that can be made to further strengthen its application in quantitative AI risk assessment.

http://arxiv.org/abs/2503.04299v1

Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search

Recent advances demonstrate that increasing inference-time computation can significantly boost the reasoning capabilities of large language models (LLMs). Although repeated sampling (i.e., generating multiple candidate outputs) is a highly effective strategy, it does not leverage external feedback signals for refinement, which are often available in tasks like coding. In this work, we propose $\textit{Adaptive Branching Monte Carlo Tree Search (AB-MCTS)}$, a novel inference-time framework that generalizes repeated sampling with principled multi-turn exploration and exploitation. At each node in the search tree, AB-MCTS dynamically decides whether to "go wider" by expanding new candidate responses or "go deeper" by revisiting existing ones based on external feedback signals. We evaluate our method on complex coding and engineering tasks using frontier models. Empirical results show that AB-MCTS consistently outperforms both repeated sampling and standard MCTS, underscoring the importance of combining the response diversity of LLMs with multi-turn solution refinement for effective inference-time scaling.

http://arxiv.org/abs/2503.04412v1

Do you want to give feedback? [yes/no]: no
you chose to not give feedback