import numpy as np
import heapq
import faiss
from sentence_transformers import SentenceTransformer
from typing import List
from rank_bm25 import BM25Okapi
from nltk.tokenize import word_tokenize

from models import Summary

class HybridRetriever:
    """
    Dense/sparse retrieval serves to retrieve K candidates out of around 150
    LLM reranking will rank the K candidates and will use the top N outputs.
    """

    def __init__(self, data: List[Summary], alpha: float = 0.8):
        """
        Initialize the hybrid retriever with the appropriate Summary objects. 
        Initialize the embedding model and BM25

        Args:
            data: list of Summary objects (retrieved via s3utils.py)
            alpha: weight for combining relevance scores. 
        """
        import nltk
        nltk.download("punkt_tab", quiet=True)

        self.data = data
        self.alpha = alpha

        paragraphs = [d.original for d in data]
        
        # Dense Retrieval Setup
        self.model = SentenceTransformer("all-MiniLM-L6-v2")
        embeddings = self.model.encode(paragraphs, show_progress_bar=True)
        faiss.normalize_L2(embeddings)
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        self.index.add(embeddings)

        # BM25 Setup
        self.bm25 = BM25Okapi([word_tokenize(p.lower()) for p in paragraphs])

    def retrieve(self, query: str, top_k: int = 15) -> List[Summary]:
        """
        Retrieve the most similar Summaries to the query using hybrid scoring

        Args:
            query: query generated by Letta
            top_k: number of Summary objects to return
        
        Returns:
            list of Summary objects
        """
        if len(self.data) <= top_k:
            return self.data
        
        # embedding scores
        query_embedding = self.model.encode([query])
        faiss.normalize_L2(query_embedding)
        emb_scores, emb_indicies = self.index.search(query_embedding, min(int(top_k * 1.5), len(self.data)))
        emb_scores = emb_scores[0]
        emb_indicies = emb_indicies[0]

        # BM25 scores
        tokenized_query = word_tokenize(query.lower())
        bm25_scores = self.bm25.get_scores(tokenized_query)
        bm25_scores = bm25_scores / (max(bm25_scores) if max(bm25_scores) > 0 else 1)

        # combine scores
        combined = []
        for i, idx in enumerate(emb_indicies):
            emb_score = emb_scores[i]
            bm25_score = bm25_scores[idx]
            combined_score = self.alpha * emb_score + (1 - self.alpha) * bm25_score
            combined.append((idx, combined_score))

        combined = heapq.nlargest(min(top_k, len(combined)), combined, key=lambda t: t[1])
        return [self.data[idx] for idx, _ in combined]
