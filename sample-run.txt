(letta) kyuds@kyuds paperbot % just run
python main.py
Successfully found agent: agent-73f44d42-0d08-4708-b1cf-da34a3db3667
Batches: 100%|████████████████████████████████████| 7/7 [00:02<00:00,  2.60it/s]
Query Response: id='message-bb929875-e71d-425e-8452-9736a50da600' date=datetime.datetime(2025, 3, 9, 11, 30, 53, tzinfo=TzInfo(UTC)) message_type='reasoning_message' reasoning='Daniel is requesting the query string once more. I should ensure it reflects his interests accurately and consistently.'

Query Response: id='message-bb929875-e71d-425e-8452-9736a50da600' date=datetime.datetime(2025, 3, 9, 11, 30, 53, tzinfo=TzInfo(UTC)) message_type='assistant_message' content='"machine learning LLM agent systems OR information retrieval OR systems architectures"'

Top 5 Response: id='message-ea8b679b-fe97-4c6c-931e-09acc686b221' date=datetime.datetime(2025, 3, 9, 11, 30, 55, tzinfo=TzInfo(UTC)) message_type='reasoning_message' reasoning="I will select the 5 most relevant papers again, focusing on Daniel's interests in LLM agent systems and information retrieval."

Top 5 Response: id='message-ea8b679b-fe97-4c6c-931e-09acc686b221' date=datetime.datetime(2025, 3, 9, 11, 30, 55, tzinfo=TzInfo(UTC)) message_type='assistant_message' content='1 2 3 4 11'

Recommended:
Measuring temporal effects of agent knowledge by date-controlled tool use

Temporal progression is an integral part of knowledge accumulation and update. Web search is frequently adopted as grounding for agent knowledge, yet its inappropriate configuration affects the quality of agent responses. Here, we construct a tool-based out-of-sample testing framework to measure the knowledge variability of large language model (LLM) agents from distinct date-controlled tools (DCTs). We demonstrate the temporal effects of an LLM agent as a writing assistant, which can use web search to help complete scientific publication abstracts. We show that temporal effects of the search engine translates into tool-dependent agent performance but can be alleviated with base model choice and explicit reasoning instructions such as chain-of-thought prompting. Our results indicate that agent evaluation should take a dynamical view and account for the temporal influence of tools and the updates of external resources.

http://arxiv.org/abs/2503.04188v1

AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems via Hierarchical Data Management

Large Language Model based multi-agent systems are revolutionizing autonomous communication and collaboration, yet they remain vulnerable to security threats like unauthorized access and data breaches. To address this, we introduce AgentSafe, a novel framework that enhances MAS security through hierarchical information management and memory protection. AgentSafe classifies information by security levels, restricting sensitive data access to authorized agents. AgentSafe incorporates two components: ThreatSieve, which secures communication by verifying information authority and preventing impersonation, and HierarCache, an adaptive memory management system that defends against unauthorized access and malicious poisoning, representing the first systematic defense for agent memory. Experiments across various LLMs show that AgentSafe significantly boosts system resilience, achieving defense success rates above 80% under adversarial conditions. Additionally, AgentSafe demonstrates scalability, maintaining robust performance as agent numbers and information complexity grow. Results underscore effectiveness of AgentSafe in securing MAS and its potential for real-world application.

http://arxiv.org/abs/2503.04392v1

IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval

We introduce IFIR, the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature. Each subset addresses one or more domain-specific retrieval tasks, replicating real-world scenarios where customized instructions are critical. IFIR enables a detailed analysis of instruction-following retrieval capabilities by incorporating instructions at different levels of complexity. We also propose a novel LLM-based evaluation method to provide a more precise and reliable assessment of model performance in following instructions. Through extensive experiments on 15 frontier retrieval models, including those based on LLMs, our results reveal that current models face significant challenges in effectively following complex, domain-specific instructions. We further provide in-depth analyses to highlight these limitations, offering valuable insights to guide future advancements in retriever development.

http://arxiv.org/abs/2503.04644v1

The Next Frontier of LLM Applications: Open Ecosystems and Hardware Synergy

Large Language Model (LLM) applications, including LLM app stores and autonomous agents, are shaping the future of AI ecosystems. However, platform silos, fragmented hardware integration, and the absence of standardized interfaces limit scalability, interoperability, and resource efficiency. While LLM app stores democratize AI, their closed ecosystems restrict modular AI reuse and cross-platform portability. Meanwhile, agent-based frameworks offer flexibility but often lack seamless integration across diverse environments. This paper envisions the future of LLM applications and proposes a three-layer decoupled architecture grounded in software engineering principles such as layered system design, service-oriented architectures, and hardware-software co-design. This architecture separates application logic, communication protocols, and hardware execution, enhancing modularity, efficiency, and cross-platform compatibility. Beyond architecture, we highlight key security and privacy challenges for safe, scalable AI deployment and outline research directions in software and security engineering. This vision aims to foster open, secure, and interoperable LLM ecosystems, guiding future advancements in AI applications.

http://arxiv.org/abs/2503.04596v1

ToolFuzz -- Automated Agent Tool Testing

Large Language Model (LLM) Agents leverage the advanced reasoning capabilities of LLMs in real-world applications. To interface with an environment, these agents often rely on tools, such as web search or database APIs. As the agent provides the LLM with tool documentation along the user query, the completeness and correctness of this documentation is critical. However, tool documentation is often over-, under-, or ill-specified, impeding the agent's accuracy. Standard software testing approaches struggle to identify these errors as they are expressed in natural language. Thus, despite its importance, there currently exists no automated method to test the tool documentation for agents. To address this issue, we present ToolFuzz, the first method for automated testing of tool documentations. ToolFuzz is designed to discover two types of errors: (1) user queries leading to tool runtime errors and (2) user queries that lead to incorrect agent responses. ToolFuzz can generate a large and diverse set of natural inputs, effectively finding tool description errors at a low false positive rate. Further, we present two straightforward prompt-engineering approaches. We evaluate all three tool testing approaches on 32 common LangChain tools and 35 newly created custom tools and 2 novel benchmarks to further strengthen the assessment. We find that many publicly available tools suffer from underspecification. Specifically, we show that ToolFuzz identifies 20x more erroneous inputs compared to the prompt-engineering approaches, making it a key component for building reliable AI agents.

http://arxiv.org/abs/2503.04479v1

Do you want to give feedback? [yes/no]: yes
Please give feedback: I really like the suggestions for the papers based on agent systems. However, I know that agents also rely a lot on prompting, so next time, could you also suggest me some relevant prompt engineering frameworks that may be related to agent systems?
Feedback Response: id='message-1ed36118-0233-49dd-a3a6-1513f783587b' date=datetime.datetime(2025, 3, 9, 11, 31, 3, tzinfo=TzInfo(UTC)) message_type='reasoning_message' reasoning='Incorporating feedback to enhance future recommendations, especially regarding prompt engineering frameworks related to agent systems.'

Feedback Response: id='message-1ed36118-0233-49dd-a3a6-1513f783587b' date=datetime.datetime(2025, 3, 9, 11, 31, 3, tzinfo=TzInfo(UTC)) message_type='tool_call_message' tool_call=ToolCall(name='core_memory_append', arguments='{\n  "label": "human",\n  "content": "Daniel likes suggestions for papers based on agent systems and wants future recommendations to include relevant prompt engineering frameworks related to agent systems.",\n  "request_heartbeat": true\n}', tool_call_id='call_F1vYM23U6ASXfUv6JoeZUhVs')

Feedback Response: id='message-5d9cb03e-5214-457a-b2ad-184b178a1a61' date=datetime.datetime(2025, 3, 9, 11, 31, 4, tzinfo=TzInfo(UTC)) message_type='tool_return_message' tool_return='None' status='success' tool_call_id='call_F1vYM23U6ASXfUv6JoeZUhVs' stdout=None stderr=None

Feedback Response: id='message-033d785f-4d76-4fdb-9498-591eedab2db7' date=datetime.datetime(2025, 3, 9, 11, 31, 6, tzinfo=TzInfo(UTC)) message_type='reasoning_message' reasoning="I have successfully updated Daniel's preferences based on his feedback. Now I can continue assisting him with more personalized recommendations."

Feedback Response: id='message-033d785f-4d76-4fdb-9498-591eedab2db7' date=datetime.datetime(2025, 3, 9, 11, 31, 6, tzinfo=TzInfo(UTC)) message_type='assistant_message' content="Thank you for the feedback! I've noted your interest in prompt engineering frameworks related to agent systems for future recommendations. Let's keep exploring those ideas together!"
